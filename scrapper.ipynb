{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 09:15:51 [INFO] (200) GET page 1\n",
      "2024-10-19 09:16:28 [INFO] (200) GET page 2\n",
      "2024-10-19 09:17:03 [INFO] (200) GET page 3\n",
      "2024-10-19 09:17:38 [INFO] (200) GET page 4\n",
      "2024-10-19 09:18:13 [INFO] (200) GET page 5\n",
      "2024-10-19 09:18:50 [INFO] (200) GET page 6\n",
      "2024-10-19 09:19:24 [INFO] (200) GET page 7\n",
      "2024-10-19 09:19:51 [INFO] (200) GET page 8\n",
      "2024-10-19 09:20:22 [INFO] (200) GET page 9\n",
      "2024-10-19 09:20:59 [INFO] (200) GET page 10\n",
      "2024-10-19 09:21:27 [INFO] (200) GET page 11\n",
      "2024-10-19 09:21:53 [INFO] (200) GET page 12\n",
      "2024-10-19 09:22:27 [INFO] (200) GET page 13\n",
      "2024-10-19 09:23:00 [INFO] (200) GET page 14\n",
      "2024-10-19 09:23:31 [INFO] (200) GET page 15\n",
      "2024-10-19 09:23:57 [INFO] (200) GET page 16\n",
      "2024-10-19 09:24:34 [INFO] (200) GET page 17\n",
      "2024-10-19 09:25:00 [INFO] (200) GET page 18\n",
      "2024-10-19 09:25:27 [INFO] (200) GET page 19\n",
      "2024-10-19 09:25:53 [INFO] (200) GET page 20\n",
      "2024-10-19 09:26:19 [INFO] (200) GET page 21\n",
      "2024-10-19 09:26:45 [INFO] (200) GET page 22\n",
      "2024-10-19 09:27:17 [INFO] (200) GET page 23\n",
      "2024-10-19 09:27:45 [INFO] (200) GET page 24\n",
      "2024-10-19 09:28:12 [INFO] (200) GET page 25\n",
      "2024-10-19 09:28:42 [INFO] (200) GET page 26\n",
      "2024-10-19 09:29:13 [INFO] (200) GET page 27\n",
      "2024-10-19 09:29:37 [INFO] (200) GET page 28\n",
      "2024-10-19 09:30:02 [INFO] (200) GET page 29\n",
      "2024-10-19 09:30:26 [INFO] (200) GET page 30\n",
      "2024-10-19 09:30:50 [INFO] (200) GET page 31\n",
      "2024-10-19 09:31:16 [INFO] (200) GET page 32\n",
      "2024-10-19 09:31:37 [INFO] (200) GET page 33\n",
      "2024-10-19 09:31:57 [INFO] (200) GET page 34\n",
      "2024-10-19 09:32:15 [INFO] (200) GET page 35\n",
      "2024-10-19 09:32:42 [INFO] (200) GET page 36\n",
      "2024-10-19 09:33:06 [INFO] (200) GET page 37\n",
      "2024-10-19 09:33:27 [INFO] (200) GET page 38\n",
      "2024-10-19 09:33:46 [INFO] (200) GET page 39\n",
      "2024-10-19 09:34:19 [INFO] (200) GET page 40\n",
      "2024-10-19 09:34:48 [INFO] (200) GET page 41\n",
      "2024-10-19 09:35:17 [INFO] (200) GET page 42\n",
      "2024-10-19 09:35:41 [INFO] (200) GET page 43\n",
      "2024-10-19 09:36:10 [INFO] (200) GET page 44\n",
      "2024-10-19 09:36:43 [INFO] (200) GET page 45\n",
      "2024-10-19 09:37:17 [INFO] (200) GET page 46\n",
      "2024-10-19 09:37:41 [INFO] (200) GET page 47\n",
      "2024-10-19 09:38:10 [INFO] (200) GET page 48\n",
      "2024-10-19 09:38:40 [INFO] (200) GET page 49\n",
      "2024-10-19 09:39:06 [INFO] (200) GET page 50\n",
      "2024-10-19 09:39:40 [INFO] (200) GET page 51\n",
      "2024-10-19 09:40:12 [INFO] (200) GET page 52\n",
      "2024-10-19 09:40:46 [INFO] (200) GET page 53\n",
      "2024-10-19 09:41:17 [INFO] (200) GET page 54\n",
      "2024-10-19 09:41:46 [INFO] (200) GET page 55\n",
      "2024-10-19 09:42:18 [INFO] (200) GET page 56\n",
      "2024-10-19 09:42:49 [INFO] (200) GET page 57\n",
      "2024-10-19 09:43:17 [INFO] (200) GET page 58\n",
      "2024-10-19 09:43:51 [INFO] (200) GET page 59\n",
      "2024-10-19 09:44:19 [INFO] (200) GET page 60\n",
      "2024-10-19 09:44:45 [INFO] (200) GET page 61\n",
      "2024-10-19 09:45:19 [INFO] (200) GET page 62\n",
      "2024-10-19 09:45:47 [INFO] (200) GET page 63\n",
      "2024-10-19 09:46:14 [INFO] (200) GET page 64\n",
      "2024-10-19 09:46:42 [INFO] (200) GET page 65\n",
      "2024-10-19 09:47:13 [INFO] (200) GET page 66\n",
      "2024-10-19 09:47:39 [INFO] (200) GET page 67\n",
      "2024-10-19 09:48:07 [INFO] (200) GET page 68\n",
      "2024-10-19 09:48:41 [INFO] (200) GET page 69\n",
      "2024-10-19 09:49:07 [INFO] (200) GET page 70\n",
      "2024-10-19 09:49:33 [INFO] (200) GET page 71\n",
      "2024-10-19 09:49:56 [INFO] (200) GET page 72\n",
      "2024-10-19 09:50:26 [INFO] (200) GET page 73\n",
      "2024-10-19 09:50:51 [INFO] (200) GET page 74\n",
      "2024-10-19 09:51:16 [INFO] (200) GET page 75\n",
      "2024-10-19 09:51:46 [INFO] (200) GET page 76\n",
      "2024-10-19 09:52:18 [INFO] (200) GET page 77\n",
      "2024-10-19 09:52:52 [INFO] (200) GET page 78\n",
      "2024-10-19 09:53:20 [INFO] (200) GET page 79\n",
      "2024-10-19 09:53:52 [INFO] (200) GET page 80\n",
      "2024-10-19 09:54:24 [INFO] (200) GET page 81\n",
      "2024-10-19 09:54:52 [INFO] (200) GET page 82\n",
      "2024-10-19 09:55:18 [INFO] (200) GET page 83\n",
      "2024-10-19 09:55:43 [INFO] (200) GET page 84\n",
      "2024-10-19 09:56:14 [INFO] (200) GET page 85\n",
      "2024-10-19 09:56:38 [INFO] (200) GET page 86\n",
      "2024-10-19 09:57:13 [INFO] (200) GET page 87\n",
      "2024-10-19 09:57:32 [INFO] (200) GET page 88\n",
      "2024-10-19 09:58:07 [INFO] (200) GET page 89\n",
      "2024-10-19 09:58:37 [INFO] (200) GET page 90\n",
      "2024-10-19 09:59:12 [INFO] (200) GET page 91\n",
      "2024-10-19 09:59:35 [INFO] (200) GET page 92\n",
      "2024-10-19 10:00:06 [INFO] (200) GET page 93\n",
      "2024-10-19 10:00:34 [INFO] (200) GET page 94\n",
      "2024-10-19 10:01:07 [INFO] (200) GET page 95\n",
      "2024-10-19 10:01:29 [INFO] (200) GET page 96\n",
      "2024-10-19 10:01:55 [INFO] (200) GET page 97\n",
      "2024-10-19 10:02:25 [INFO] (200) GET page 98\n",
      "2024-10-19 10:02:56 [INFO] (200) GET page 99\n",
      "2024-10-19 10:03:25 [INFO] (200) GET page 100\n",
      "2024-10-19 10:03:49 [INFO] (200) GET page 101\n",
      "2024-10-19 10:04:13 [INFO] (200) GET page 102\n",
      "2024-10-19 10:04:35 [INFO] (200) GET page 103\n",
      "2024-10-19 10:05:14 [INFO] (200) GET page 104\n",
      "2024-10-19 10:05:43 [INFO] (200) GET page 105\n",
      "2024-10-19 10:06:05 [INFO] (200) GET page 106\n",
      "2024-10-19 10:06:35 [INFO] (200) GET page 107\n",
      "2024-10-19 10:07:08 [INFO] (200) GET page 108\n",
      "2024-10-19 10:07:40 [INFO] (200) GET page 109\n",
      "2024-10-19 10:08:06 [INFO] (200) GET page 110\n",
      "2024-10-19 10:08:37 [INFO] (200) GET page 111\n",
      "2024-10-19 10:09:19 [INFO] (200) GET page 112\n",
      "2024-10-19 10:09:43 [INFO] (200) GET page 113\n",
      "2024-10-19 10:10:10 [INFO] (200) GET page 114\n",
      "2024-10-19 10:10:37 [INFO] (200) GET page 115\n",
      "2024-10-19 10:11:02 [INFO] (200) GET page 116\n",
      "2024-10-19 10:11:24 [INFO] (200) GET page 117\n",
      "2024-10-19 10:11:51 [INFO] (200) GET page 118\n",
      "2024-10-19 10:12:24 [INFO] (200) GET page 119\n",
      "2024-10-19 10:12:49 [INFO] (200) GET page 120\n",
      "2024-10-19 10:13:17 [INFO] (200) GET page 121\n",
      "2024-10-19 10:13:36 [INFO] (200) GET page 122\n",
      "2024-10-19 10:14:01 [INFO] (200) GET page 123\n",
      "2024-10-19 10:14:20 [INFO] (200) GET page 124\n",
      "2024-10-19 10:14:49 [INFO] (200) GET page 125\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Mengatur logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Header untuk user agent agar mirip dengan browser\n",
    "user_agent = (\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) '\n",
    "    'AppleWebKit/537.36 (KHTML, seperti Gecko) '\n",
    "    'Chrome/106.0.0.0 Safari/537.36'\n",
    ")\n",
    "\n",
    "default_headers = {'User-Agent': user_agent}\n",
    "\n",
    "def get_max_page() -> int:\n",
    "    \"\"\" Mendapatkan jumlah maksimum halaman \"\"\"\n",
    "    session = requests.Session()\n",
    "    response = session.get('https://pergikuliner.com/restaurants?utf8=%E2%9C%93&search_place=&default_search=Jakarta&search_name_cuisine=kafe&commit=')\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = page.find('h2', {'id': 'top-total-search-view'})\n",
    "    text = text.find('strong').text.split('dari')\n",
    "    page_num = [int(t.strip()) for t in text]\n",
    "    return int(page_num[1] / page_num[0])\n",
    "\n",
    "def scrape_page(response):\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    cards = page.find_all('div', class_='restaurant-result-wrapper')\n",
    "    data = []\n",
    "    for item in cards:\n",
    "        # judul dan tautan\n",
    "        title = item.find('h3').text.strip()\n",
    "        link = 'https://pergikuliner.com' + item.find('a')['href']\n",
    "        # jenis makanan dan lokasi\n",
    "        description = item.find('div', class_='item-group').find('div').text.strip()\n",
    "        description = description.split('|')\n",
    "        if len(description) > 1:\n",
    "            location = description[0].strip()\n",
    "            cuisine = [i.strip() for i in description[1].split(',')]\n",
    "        else:\n",
    "            location = description\n",
    "            cuisine = None\n",
    "        # peringkat\n",
    "        full_rate = item.find('div', class_='item-rating-result').find('small').text.strip()\n",
    "        rate = item.find('div', class_='item-rating-result').text.replace(full_rate, '').strip()\n",
    "        full_rate = full_rate.replace('/', '')\n",
    "        # lokasi dan harga\n",
    "        for p in item.find_all('p', class_='clearfix'):\n",
    "            if 'icon-map' in p.find('i')['class']:\n",
    "                place = p.find_all('span', class_='truncate')\n",
    "                address = place[0].text.strip()\n",
    "                street = place[1].text.strip()\n",
    "            elif 'icon-price' in p.find('i')['class']:\n",
    "                price_text = p.find('span').text.strip()\n",
    "                if re.findall(r'-', price_text):\n",
    "                    price_from = price_text.split('-')[0].strip()\n",
    "                    price_till = price_text.split('-')[1].replace('/orang', '').strip()\n",
    "                elif re.findall(r'Di atas', price_text):\n",
    "                    price_from = price_text.replace('Di atas', '').replace('/orang', '').strip()\n",
    "                    price_till = None\n",
    "                elif re.findall(r'Di bawah', price_text):\n",
    "                    price_from = 'Rp. 0'\n",
    "                    price_till = price_text.replace('Di bawah', '').replace('/orang', '').strip()\n",
    "                else:\n",
    "                    logging.info(f\"Kondisi lain pada harga\")\n",
    "                    price_from = price_text\n",
    "                    price_till = price_text\n",
    "            else:\n",
    "                logging.info(f\"Bagian lain di lokasi dan harga\")\n",
    "\n",
    "        item_data = {\n",
    "            'title': title,\n",
    "            'rate': rate,\n",
    "            'cuisine': cuisine,\n",
    "            'location': location,\n",
    "            'address': address,\n",
    "            'street': street,\n",
    "            'price_from': price_from,\n",
    "            'price_till': price_till,\n",
    "            'url': link,\n",
    "        }\n",
    "        data.append(item_data)\n",
    "    return data\n",
    "\n",
    "def scrape_restaurant_details(link):\n",
    "    \"\"\" Mengambil detail tambahan dari halaman restoran \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update(default_headers)\n",
    "    try:\n",
    "        response = session.get(link)\n",
    "        if response.status_code == 200:\n",
    "            page = BeautifulSoup(response.text, 'html.parser')\n",
    "            additional_info = {}\n",
    "            \n",
    "            # Mengambil deskripsi restoran\n",
    "            description = page.find('div', class_='restaurant-description')\n",
    "            additional_info['description'] = description.text.strip() jika description else None\n",
    "            \n",
    "            # Mengambil jam buka\n",
    "            opening_hours = page.find('time', itemprop='openingHours')\n",
    "            additional_info['opening_hours'] = opening_hours.text.strip() jika opening_hours else None\n",
    "            \n",
    "            # Memeriksa ketersediaan Wi-Fi\n",
    "            wifi_checkbox = page.find('input', {'type': 'checkbox', 'name': 'wifi'})\n",
    "            additional_info['wifi_available'] = wifi_checkbox.get('checked') is not None jika wifi_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan area merokok\n",
    "            smoking_area_checkbox = page.find('input', {'type': 'checkbox', 'name': 'smoking_area'})\n",
    "            additional_info['smoking_area_available'] = smoking_area_checkbox.get('checked') is not None jika smoking_area_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan tempat duduk luar ruangan\n",
    "            outdoor_seat_checkbox = page.find('input', {'type': 'checkbox', 'name': 'outdoor_seat'})\n",
    "            additional_info['outdoor_seat_available'] = outdoor_seat_checkbox.get('checked') is not None jika outdoor_seat_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan layanan full-time\n",
    "            full_time_checkbox = page.find('input', {'type': 'checkbox', 'name': 'full_time'})\n",
    "            additional_info['full_time_available'] = full_time_checkbox.get('checked') is not None jika full_time_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan ruang VIP\n",
    "            vip_room_checkbox = page.find('input', {'type': 'checkbox', 'name': 'vip_room'})\n",
    "            additional_info['vip_room_available'] = vip_room_checkbox.get('checked') is not None jika vip_room_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan reservasi\n",
    "            reservation_checkbox = page.find('input', {'type': 'checkbox', 'name': 'reservation'})\n",
    "            additional_info['reservation_available'] = reservation_checkbox.get('checked') is not None jika reservation_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan area parkir\n",
    "            parking_area_checkbox = page.find('input', {'type': 'checkbox', 'name': 'parking_area'})\n",
    "            additional_info['parking_area_available'] = parking_area_checkbox.get('checked') is not None jika parking_area_checkbox else False\n",
    "            \n",
    "            # Mengambil review body untuk review_11, review_12, dan review_13\n",
    "            for review_id in ['review_11', 'review_12', 'review_13']:\n",
    "                review_body = page.find('div', {'id': review_id, 'itemprop': 'reviewBody'})\n",
    "                additional_info[review_id] = review_body.text.strip() jika review_body else 'None'\n",
    "            \n",
    "            # Pastikan jika review_11 ada, review_12 dan review_13 di-set ke 'None' jika tidak ditemukan\n",
    "            jika additional_info['review_11'] == 'None':\n",
    "                additional_info['review_12'] = 'None'\n",
    "                additional_info['review_13'] = 'None'\n",
    "                \n",
    "            return additional_info\n",
    "        else:\n",
    "            logging.warning(f\"Gagal mengambil detail untuk {link} (Status Code: {response.status_code})\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Kesalahan mengambil detail untuk {link}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def crawl(npage=None):\n",
    "    session = requests.Session()\n",
    "    session.headers.update(default_headers)\n",
    "\n",
    "    jika npage is None:\n",
    "        npage = get_max_page() + 1\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for n in range(1, npage):\n",
    "        params = {'page': n}\n",
    "        try:\n",
    "            response = session.get('https://pergikuliner.com/restaurants?utf8=%E2%9C%93&search_place=&default_search=Jakarta&search_name_cuisine=kafe&commit=', params=params)\n",
    "            logging.info(f\"({response.status_code}) GET halaman {n}\")\n",
    "            page_data = scrape_page(response)\n",
    "            for restaurant in page_data:\n",
    "                # Mengambil detail tambahan untuk setiap restoran\n",
    "                additional_data = scrape_restaurant_details(restaurant['url'])\n",
    "                restaurant.update(additional_data)\n",
    "            data += page_data\n",
    "            sleep(1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Kesalahan pada {n}: {e}\")\n",
    "            pass\n",
    "    return data\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = crawl()\n",
    "    save_data(data, \"Jakarta.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSON file into DataFrame\n",
    "df = pd.read_json('Jakarta.json')\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('Jakarta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 10:15:16 [INFO] (200) GET page 1\n",
      "2024-10-19 10:15:34 [INFO] (200) GET page 2\n",
      "2024-10-19 10:15:55 [INFO] (200) GET page 3\n",
      "2024-10-19 10:16:17 [INFO] (200) GET page 4\n",
      "2024-10-19 10:16:35 [INFO] (200) GET page 5\n",
      "2024-10-19 10:16:51 [INFO] (200) GET page 6\n",
      "2024-10-19 10:17:11 [INFO] (200) GET page 7\n",
      "2024-10-19 10:17:29 [INFO] (200) GET page 8\n",
      "2024-10-19 10:17:45 [INFO] (200) GET page 9\n",
      "2024-10-19 10:18:06 [INFO] (200) GET page 10\n",
      "2024-10-19 10:18:18 [INFO] (200) GET page 11\n",
      "2024-10-19 10:18:35 [INFO] (200) GET page 12\n",
      "2024-10-19 10:18:51 [INFO] (200) GET page 13\n",
      "2024-10-19 10:19:05 [INFO] (200) GET page 14\n",
      "2024-10-19 10:19:23 [INFO] (200) GET page 15\n",
      "2024-10-19 10:19:37 [INFO] (200) GET page 16\n",
      "2024-10-19 10:19:53 [INFO] (200) GET page 17\n",
      "2024-10-19 10:20:12 [INFO] (200) GET page 18\n",
      "2024-10-19 10:20:28 [INFO] (200) GET page 19\n",
      "2024-10-19 10:20:39 [INFO] (200) GET page 20\n",
      "2024-10-19 10:20:52 [INFO] (200) GET page 21\n",
      "2024-10-19 10:21:06 [INFO] (200) GET page 22\n",
      "2024-10-19 10:21:23 [INFO] (200) GET page 23\n",
      "2024-10-19 10:21:33 [INFO] (200) GET page 24\n",
      "2024-10-19 10:21:49 [INFO] (200) GET page 25\n",
      "2024-10-19 10:22:04 [INFO] (200) GET page 26\n",
      "2024-10-19 10:22:18 [INFO] (200) GET page 27\n",
      "2024-10-19 10:22:37 [INFO] (200) GET page 28\n",
      "2024-10-19 10:22:49 [INFO] (200) GET page 29\n",
      "2024-10-19 10:23:05 [INFO] (200) GET page 30\n",
      "2024-10-19 10:23:21 [INFO] (200) GET page 31\n",
      "2024-10-19 10:23:33 [INFO] (200) GET page 32\n",
      "2024-10-19 10:23:47 [INFO] (200) GET page 33\n",
      "2024-10-19 10:23:59 [INFO] (200) GET page 34\n",
      "2024-10-19 10:24:11 [INFO] (200) GET page 35\n",
      "2024-10-19 10:24:21 [INFO] (200) GET page 36\n",
      "2024-10-19 10:24:33 [INFO] (200) GET page 37\n",
      "2024-10-19 10:24:43 [INFO] (200) GET page 38\n",
      "2024-10-19 10:24:56 [INFO] (200) GET page 39\n",
      "2024-10-19 10:25:09 [INFO] (200) GET page 40\n",
      "2024-10-19 10:25:27 [INFO] (200) GET page 41\n",
      "2024-10-19 10:25:37 [INFO] (200) GET page 42\n",
      "2024-10-19 10:25:47 [INFO] (200) GET page 43\n",
      "2024-10-19 10:25:57 [INFO] (200) GET page 44\n",
      "2024-10-19 10:26:08 [INFO] (200) GET page 45\n",
      "2024-10-19 10:26:18 [INFO] (200) GET page 46\n",
      "2024-10-19 10:26:30 [INFO] (200) GET page 47\n",
      "2024-10-19 10:26:41 [INFO] (200) GET page 48\n",
      "2024-10-19 10:26:51 [INFO] (200) GET page 49\n",
      "2024-10-19 10:27:01 [INFO] (200) GET page 50\n",
      "2024-10-19 10:27:10 [INFO] (200) GET page 51\n",
      "2024-10-19 10:27:19 [INFO] (200) GET page 52\n",
      "2024-10-19 10:27:30 [INFO] (200) GET page 53\n",
      "2024-10-19 10:27:39 [INFO] (200) GET page 54\n",
      "2024-10-19 10:27:51 [INFO] (200) GET page 55\n",
      "2024-10-19 10:28:01 [INFO] (200) GET page 56\n",
      "2024-10-19 10:28:09 [INFO] (200) GET page 57\n",
      "2024-10-19 10:28:19 [INFO] (200) GET page 58\n",
      "2024-10-19 10:28:25 [INFO] (200) GET page 59\n",
      "2024-10-19 10:28:32 [INFO] (200) GET page 60\n",
      "2024-10-19 10:28:37 [INFO] (200) GET page 61\n",
      "2024-10-19 10:28:43 [INFO] (200) GET page 62\n",
      "2024-10-19 10:28:49 [INFO] (200) GET page 63\n",
      "2024-10-19 10:28:55 [INFO] (200) GET page 64\n",
      "2024-10-19 10:29:02 [INFO] (200) GET page 65\n",
      "2024-10-19 10:29:07 [INFO] (200) GET page 66\n",
      "2024-10-19 10:29:13 [INFO] (200) GET page 67\n",
      "2024-10-19 10:29:19 [INFO] (200) GET page 68\n",
      "2024-10-19 10:29:24 [INFO] (200) GET page 69\n",
      "2024-10-19 10:29:30 [INFO] (200) GET page 70\n",
      "2024-10-19 10:29:36 [INFO] (200) GET page 71\n",
      "2024-10-19 10:29:41 [INFO] (200) GET page 72\n",
      "2024-10-19 10:29:47 [INFO] (200) GET page 73\n",
      "2024-10-19 10:29:53 [INFO] (200) GET page 74\n",
      "2024-10-19 10:29:58 [INFO] (200) GET page 75\n",
      "2024-10-19 10:30:04 [INFO] (200) GET page 76\n",
      "2024-10-19 10:30:10 [INFO] (200) GET page 77\n",
      "2024-10-19 10:30:16 [INFO] (200) GET page 78\n",
      "2024-10-19 10:30:23 [INFO] (200) GET page 79\n",
      "2024-10-19 10:30:28 [INFO] (200) GET page 80\n",
      "2024-10-19 10:30:34 [INFO] (200) GET page 81\n",
      "2024-10-19 10:30:40 [INFO] (200) GET page 82\n",
      "2024-10-19 10:30:45 [INFO] (200) GET page 83\n",
      "2024-10-19 10:30:51 [INFO] (200) GET page 84\n",
      "2024-10-19 10:30:57 [INFO] (200) GET page 85\n",
      "2024-10-19 10:31:03 [INFO] (200) GET page 86\n",
      "2024-10-19 10:31:08 [INFO] (200) GET page 87\n",
      "2024-10-19 10:31:14 [INFO] (200) GET page 88\n",
      "2024-10-19 10:31:21 [INFO] (200) GET page 89\n",
      "2024-10-19 10:31:26 [INFO] (200) GET page 90\n",
      "2024-10-19 10:31:33 [INFO] (200) GET page 91\n",
      "2024-10-19 10:31:39 [INFO] (200) GET page 92\n",
      "2024-10-19 10:31:45 [INFO] (200) GET page 93\n",
      "2024-10-19 10:31:51 [INFO] (200) GET page 94\n",
      "2024-10-19 10:31:58 [INFO] (200) GET page 95\n",
      "2024-10-19 10:32:04 [INFO] (200) GET page 96\n",
      "2024-10-19 10:32:10 [INFO] (200) GET page 97\n",
      "2024-10-19 10:32:16 [INFO] (200) GET page 98\n",
      "2024-10-19 10:32:22 [INFO] (200) GET page 99\n",
      "2024-10-19 10:32:29 [INFO] (200) GET page 100\n",
      "2024-10-19 10:32:35 [INFO] (200) GET page 101\n",
      "2024-10-19 10:32:42 [INFO] (200) GET page 102\n",
      "2024-10-19 10:32:48 [INFO] (200) GET page 103\n",
      "2024-10-19 10:32:55 [INFO] (200) GET page 104\n",
      "2024-10-19 10:33:02 [INFO] (200) GET page 105\n",
      "2024-10-19 10:33:08 [INFO] (200) GET page 106\n",
      "2024-10-19 10:33:14 [INFO] (200) GET page 107\n",
      "2024-10-19 10:33:21 [INFO] (200) GET page 108\n",
      "2024-10-19 10:33:27 [INFO] (200) GET page 109\n",
      "2024-10-19 10:33:33 [INFO] (200) GET page 110\n",
      "2024-10-19 10:33:39 [INFO] (200) GET page 111\n",
      "2024-10-19 10:33:45 [INFO] (200) GET page 112\n",
      "2024-10-19 10:33:51 [INFO] (200) GET page 113\n",
      "2024-10-19 10:33:57 [INFO] (200) GET page 114\n",
      "2024-10-19 10:34:03 [INFO] (200) GET page 115\n",
      "2024-10-19 10:34:09 [INFO] (200) GET page 116\n",
      "2024-10-19 10:34:15 [INFO] (200) GET page 117\n",
      "2024-10-19 10:34:21 [INFO] (200) GET page 118\n",
      "2024-10-19 10:34:27 [INFO] (200) GET page 119\n",
      "2024-10-19 10:34:33 [INFO] (200) GET page 120\n",
      "2024-10-19 10:34:39 [INFO] (200) GET page 121\n",
      "2024-10-19 10:34:46 [INFO] (200) GET page 122\n",
      "2024-10-19 10:34:52 [INFO] (200) GET page 123\n",
      "2024-10-19 10:34:58 [INFO] (200) GET page 124\n",
      "2024-10-19 10:35:06 [INFO] (200) GET page 125\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Mengatur logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Header untuk user agent agar mirip dengan browser\n",
    "user_agent = (\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) '\n",
    "    'AppleWebKit/537.36 (KHTML, seperti Gecko) '\n",
    "    'Chrome/106.0.0.0 Safari/537.36'\n",
    ")\n",
    "\n",
    "default_headers = {'User-Agent': user_agent}\n",
    "\n",
    "def get_max_page() -> int:\n",
    "    \"\"\" Mendapatkan jumlah maksimum halaman \"\"\"\n",
    "    session = requests.Session()\n",
    "    response = session.get('https://pergikuliner.com/restaurants?utf8=%E2%9C%93&search_place=&default_search=Surabaya&search_name_cuisine=kafe&commit=')\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = page.find('h2', {'id': 'top-total-search-view'})\n",
    "    text = text.find('strong').text.split('dari')\n",
    "    page_num = [int(t.strip()) for t in text]\n",
    "    return int(page_num[1] / page_num[0])\n",
    "\n",
    "def scrape_page(response):\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    cards = page.find_all('div', class_='restaurant-result-wrapper')\n",
    "    data = []\n",
    "    for item in cards:\n",
    "        # judul dan tautan\n",
    "        title = item.find('h3').text.strip()\n",
    "        link = 'https://pergikuliner.com' + item.find('a')['href']\n",
    "        # jenis makanan dan lokasi\n",
    "        description = item.find('div', class_='item-group').find('div').text.strip()\n",
    "        description = description.split('|')\n",
    "        if len(description) > 1:\n",
    "            location = description[0].strip()\n",
    "            cuisine = [i.strip() for i in description[1].split(',')]\n",
    "        else:\n",
    "            location = description\n",
    "            cuisine = None\n",
    "        # peringkat\n",
    "        full_rate = item.find('div', class_='item-rating-result').find('small').text.strip()\n",
    "        rate = item.find('div', class_='item-rating-result').text.replace(full_rate, '').strip()\n",
    "        full_rate = full_rate.replace('/', '')\n",
    "        # lokasi dan harga\n",
    "        for p in item.find_all('p', class_='clearfix'):\n",
    "            if 'icon-map' in p.find('i')['class']:\n",
    "                place = p.find_all('span', class_='truncate')\n",
    "                address = place[0].text.strip()\n",
    "                street = place[1].text.strip()\n",
    "            elif 'icon-price' in p.find('i')['class']:\n",
    "                price_text = p.find('span').text.strip()\n",
    "                if re.findall(r'-', price_text):\n",
    "                    price_from = price_text.split('-')[0].strip()\n",
    "                    price_till = price_text.split('-')[1].replace('/orang', '').strip()\n",
    "                elif re.findall(r'Di atas', price_text):\n",
    "                    price_from = price_text.replace('Di atas', '').replace('/orang', '').strip()\n",
    "                    price_till = None\n",
    "                elif re.findall(r'Di bawah', price_text):\n",
    "                    price_from = 'Rp. 0'\n",
    "                    price_till = price_text.replace('Di bawah', '').replace('/orang', '').strip()\n",
    "                else:\n",
    "                    logging.info(f\"Kondisi lain pada harga\")\n",
    "                    price_from = price_text\n",
    "                    price_till = price_text\n",
    "            else:\n",
    "                logging.info(f\"Bagian lain di lokasi dan harga\")\n",
    "\n",
    "        item_data = {\n",
    "            'title': title,\n",
    "            'rate': rate,\n",
    "            'cuisine': cuisine,\n",
    "            'location': location,\n",
    "            'address': address,\n",
    "            'street': street,\n",
    "            'price_from': price_from,\n",
    "            'price_till': price_till,\n",
    "            'url': link,\n",
    "        }\n",
    "        data.append(item_data)\n",
    "    return data\n",
    "\n",
    "def scrape_restaurant_details(link):\n",
    "    \"\"\" Mengambil detail tambahan dari halaman restoran \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update(default_headers)\n",
    "    try:\n",
    "        response = session.get(link)\n",
    "        if response.status_code == 200:\n",
    "            page = BeautifulSoup(response.text, 'html.parser')\n",
    "            additional_info = {}\n",
    "            \n",
    "            # Mengambil deskripsi restoran\n",
    "            description = page.find('div', class_='restaurant-description')\n",
    "            additional_info['description'] = description.text.strip() jika description else None\n",
    "            \n",
    "            # Mengambil jam buka\n",
    "            opening_hours = page.find('time', itemprop='openingHours')\n",
    "            additional_info['opening_hours'] = opening_hours.text.strip() jika opening_hours else None\n",
    "            \n",
    "            # Memeriksa ketersediaan Wi-Fi\n",
    "            wifi_checkbox = page.find('input', {'type': 'checkbox', 'name': 'wifi'})\n",
    "            additional_info['wifi_available'] = wifi_checkbox.get('checked') is not None jika wifi_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan area merokok\n",
    "            smoking_area_checkbox = page.find('input', {'type': 'checkbox', 'name': 'smoking_area'})\n",
    "            additional_info['smoking_area_available'] = smoking_area_checkbox.get('checked') is not None jika smoking_area_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan tempat duduk luar ruangan\n",
    "            outdoor_seat_checkbox = page.find('input', {'type': 'checkbox', 'name': 'outdoor_seat'})\n",
    "            additional_info['outdoor_seat_available'] = outdoor_seat_checkbox.get('checked') is not None jika outdoor_seat_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan layanan full-time\n",
    "            full_time_checkbox = page.find('input', {'type': 'checkbox', 'name': 'full_time'})\n",
    "            additional_info['full_time_available'] = full_time_checkbox.get('checked') is not None jika full_time_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan ruang VIP\n",
    "            vip_room_checkbox = page.find('input', {'type': 'checkbox', 'name': 'vip_room'})\n",
    "            additional_info['vip_room_available'] = vip_room_checkbox.get('checked') is not None jika vip_room_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan reservasi\n",
    "            reservation_checkbox = page.find('input', {'type': 'checkbox', 'name': 'reservation'})\n",
    "            additional_info['reservation_available'] = reservation_checkbox.get('checked') is not None jika reservation_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan area parkir\n",
    "            parking_area_checkbox = page.find('input', {'type': 'checkbox', 'name': 'parking_area'})\n",
    "            additional_info['parking_area_available'] = parking_area_checkbox.get('checked') is not None jika parking_area_checkbox else False\n",
    "            \n",
    "            # Mengambil review body untuk review_11, review_12, dan review_13\n",
    "            for review_id in ['review_11', 'review_12', 'review_13']:\n",
    "                review_body = page.find('div', {'id': review_id, 'itemprop': 'reviewBody'})\n",
    "                additional_info[review_id] = review_body.text.strip() jika review_body else 'None'\n",
    "            \n",
    "            # Pastikan jika review_11 ada, review_12 dan review_13 di-set ke 'None' jika tidak ditemukan\n",
    "            jika additional_info['review_11'] == 'None':\n",
    "                additional_info['review_12'] = 'None'\n",
    "                additional_info['review_13'] = 'None'\n",
    "                \n",
    "            return additional_info\n",
    "        else:\n",
    "            logging.warning(f\"Gagal mengambil detail untuk {link} (Status Code: {response.status_code})\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Kesalahan mengambil detail untuk {link}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def crawl(npage=None):\n",
    "    session = requests.Session()\n",
    "    session.headers.update(default_headers)\n",
    "\n",
    "    jika npage is None:\n",
    "        npage = get_max_page() + 1\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for n in range(1, npage):\n",
    "        params = {'page': n}\n",
    "        try:\n",
    "            response = session.get('https://pergikuliner.com/restaurants?utf8=%E2%9C%93&search_place=&default_search=Surabaya&search_name_cuisine=kafe&commit=', params=params)\n",
    "            logging.info(f\"({response.status_code}) GET halaman {n}\")\n",
    "            page_data = scrape_page(response)\n",
    "            for restaurant in page_data:\n",
    "                # Mengambil detail tambahan untuk setiap restoran\n",
    "                additional_data = scrape_restaurant_details(restaurant['url'])\n",
    "                restaurant.update(additional_data)\n",
    "            data += page_data\n",
    "            sleep(1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Kesalahan pada {n}: {e}\")\n",
    "            pass\n",
    "    return data\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = crawl()\n",
    "    save_data(data, \"Surabaya.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSON file into DataFrame\n",
    "df = pd.read_json('surabaya.json')\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('surabaya.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 10:35:14 [INFO] (200) GET page 1\n",
      "2024-10-19 10:35:44 [INFO] (200) GET page 2\n",
      "2024-10-19 10:36:30 [INFO] (200) GET page 3\n",
      "2024-10-19 10:36:58 [INFO] (200) GET page 4\n",
      "2024-10-19 10:37:25 [INFO] (200) GET page 5\n",
      "2024-10-19 10:37:50 [INFO] (200) GET page 6\n",
      "2024-10-19 10:38:35 [INFO] (200) GET page 7\n",
      "2024-10-19 10:39:01 [INFO] (200) GET page 8\n",
      "2024-10-19 10:39:41 [INFO] (200) GET page 9\n",
      "2024-10-19 10:40:07 [INFO] (200) GET page 10\n",
      "2024-10-19 10:40:29 [INFO] (200) GET page 11\n",
      "2024-10-19 10:40:52 [INFO] (200) GET page 12\n",
      "2024-10-19 10:41:12 [INFO] (200) GET page 13\n",
      "2024-10-19 10:41:35 [INFO] (200) GET page 14\n",
      "2024-10-19 10:41:58 [INFO] (200) GET page 15\n",
      "2024-10-19 10:42:23 [INFO] (200) GET page 16\n",
      "2024-10-19 10:42:45 [INFO] (200) GET page 17\n",
      "2024-10-19 10:43:05 [INFO] (200) GET page 18\n",
      "2024-10-19 10:43:36 [INFO] (200) GET page 19\n",
      "2024-10-19 10:44:08 [INFO] (200) GET page 20\n",
      "2024-10-19 10:44:30 [INFO] (200) GET page 21\n",
      "2024-10-19 10:44:55 [INFO] (200) GET page 22\n",
      "2024-10-19 10:45:21 [INFO] (200) GET page 23\n",
      "2024-10-19 10:45:43 [INFO] (200) GET page 24\n",
      "2024-10-19 10:46:05 [INFO] (200) GET page 25\n",
      "2024-10-19 10:46:30 [INFO] (200) GET page 26\n",
      "2024-10-19 10:46:48 [INFO] (200) GET page 27\n",
      "2024-10-19 10:47:12 [INFO] (200) GET page 28\n",
      "2024-10-19 10:47:35 [INFO] (200) GET page 29\n",
      "2024-10-19 10:47:58 [INFO] (200) GET page 30\n",
      "2024-10-19 10:48:16 [INFO] (200) GET page 31\n",
      "2024-10-19 10:48:39 [INFO] (200) GET page 32\n",
      "2024-10-19 10:48:59 [INFO] (200) GET page 33\n",
      "2024-10-19 10:49:18 [INFO] (200) GET page 34\n",
      "2024-10-19 10:49:35 [INFO] (200) GET page 35\n",
      "2024-10-19 10:49:57 [INFO] (200) GET page 36\n",
      "2024-10-19 10:50:26 [INFO] (200) GET page 37\n",
      "2024-10-19 10:50:51 [INFO] (200) GET page 38\n",
      "2024-10-19 10:51:16 [INFO] (200) GET page 39\n",
      "2024-10-19 10:51:33 [INFO] (200) GET page 40\n",
      "2024-10-19 10:51:50 [INFO] (200) GET page 41\n",
      "2024-10-19 10:52:06 [INFO] (200) GET page 42\n",
      "2024-10-19 10:52:23 [INFO] (200) GET page 43\n",
      "2024-10-19 10:52:45 [INFO] (200) GET page 44\n",
      "2024-10-19 10:53:02 [INFO] (200) GET page 45\n",
      "2024-10-19 10:53:30 [INFO] (200) GET page 46\n",
      "2024-10-19 10:53:46 [INFO] (200) GET page 47\n",
      "2024-10-19 10:54:07 [INFO] (200) GET page 48\n",
      "2024-10-19 10:54:25 [INFO] (200) GET page 49\n",
      "2024-10-19 10:54:41 [INFO] (200) GET page 50\n",
      "2024-10-19 10:54:58 [INFO] (200) GET page 51\n",
      "2024-10-19 10:55:13 [INFO] (200) GET page 52\n",
      "2024-10-19 10:55:30 [INFO] (200) GET page 53\n",
      "2024-10-19 10:55:47 [INFO] (200) GET page 54\n",
      "2024-10-19 10:55:56 [INFO] (200) GET page 55\n",
      "2024-10-19 10:56:10 [INFO] (200) GET page 56\n",
      "2024-10-19 10:56:27 [INFO] (200) GET page 57\n",
      "2024-10-19 10:56:45 [INFO] (200) GET page 58\n",
      "2024-10-19 10:57:08 [INFO] (200) GET page 59\n",
      "2024-10-19 10:57:25 [INFO] (200) GET page 60\n",
      "2024-10-19 10:57:47 [INFO] (200) GET page 61\n",
      "2024-10-19 10:58:01 [INFO] (200) GET page 62\n",
      "2024-10-19 10:58:26 [INFO] (200) GET page 63\n",
      "2024-10-19 10:58:51 [INFO] (200) GET page 64\n",
      "2024-10-19 10:59:04 [INFO] (200) GET page 65\n",
      "2024-10-19 10:59:20 [INFO] (200) GET page 66\n",
      "2024-10-19 10:59:42 [INFO] (200) GET page 67\n",
      "2024-10-19 11:00:01 [INFO] (200) GET page 68\n",
      "2024-10-19 11:00:19 [INFO] (200) GET page 69\n",
      "2024-10-19 11:00:32 [INFO] (200) GET page 70\n",
      "2024-10-19 11:00:52 [INFO] (200) GET page 71\n",
      "2024-10-19 11:01:15 [INFO] (200) GET page 72\n",
      "2024-10-19 11:01:33 [INFO] (200) GET page 73\n",
      "2024-10-19 11:01:50 [INFO] (200) GET page 74\n",
      "2024-10-19 11:02:15 [INFO] (200) GET page 75\n",
      "2024-10-19 11:02:38 [INFO] (200) GET page 76\n",
      "2024-10-19 11:02:53 [INFO] (200) GET page 77\n",
      "2024-10-19 11:03:14 [INFO] (200) GET page 78\n",
      "2024-10-19 11:03:33 [INFO] (200) GET page 79\n",
      "2024-10-19 11:03:59 [INFO] (200) GET page 80\n",
      "2024-10-19 11:04:16 [INFO] (200) GET page 81\n",
      "2024-10-19 11:04:32 [INFO] (200) GET page 82\n",
      "2024-10-19 11:04:43 [INFO] (200) GET page 83\n",
      "2024-10-19 11:04:55 [INFO] (200) GET page 84\n",
      "2024-10-19 11:05:12 [INFO] (200) GET page 85\n",
      "2024-10-19 11:05:23 [INFO] (200) GET page 86\n",
      "2024-10-19 11:05:35 [INFO] (200) GET page 87\n",
      "2024-10-19 11:05:51 [INFO] (200) GET page 88\n",
      "2024-10-19 11:06:07 [INFO] (200) GET page 89\n",
      "2024-10-19 11:06:25 [INFO] (200) GET page 90\n",
      "2024-10-19 11:06:44 [INFO] (200) GET page 91\n",
      "2024-10-19 11:06:59 [INFO] (200) GET page 92\n",
      "2024-10-19 11:07:18 [INFO] (200) GET page 93\n",
      "2024-10-19 11:07:30 [INFO] (200) GET page 94\n",
      "2024-10-19 11:07:44 [INFO] (200) GET page 95\n",
      "2024-10-19 11:07:57 [INFO] (200) GET page 96\n",
      "2024-10-19 11:08:10 [INFO] (200) GET page 97\n",
      "2024-10-19 11:08:24 [INFO] (200) GET page 98\n",
      "2024-10-19 11:08:40 [INFO] (200) GET page 99\n",
      "2024-10-19 11:08:52 [INFO] (200) GET page 100\n",
      "2024-10-19 11:09:10 [INFO] (200) GET page 101\n",
      "2024-10-19 11:09:22 [INFO] (200) GET page 102\n",
      "2024-10-19 11:09:32 [INFO] (200) GET page 103\n",
      "2024-10-19 11:09:45 [INFO] (200) GET page 104\n",
      "2024-10-19 11:09:55 [INFO] (200) GET page 105\n",
      "2024-10-19 11:10:07 [INFO] (200) GET page 106\n",
      "2024-10-19 11:10:22 [INFO] (200) GET page 107\n",
      "2024-10-19 11:10:41 [INFO] (200) GET page 108\n",
      "2024-10-19 11:10:56 [INFO] (200) GET page 109\n",
      "2024-10-19 11:11:13 [INFO] (200) GET page 110\n",
      "2024-10-19 11:11:27 [INFO] (200) GET page 111\n",
      "2024-10-19 11:11:38 [INFO] (200) GET page 112\n",
      "2024-10-19 11:11:49 [INFO] (200) GET page 113\n",
      "2024-10-19 11:12:01 [INFO] (200) GET page 114\n",
      "2024-10-19 11:12:14 [INFO] (200) GET page 115\n",
      "2024-10-19 11:12:24 [INFO] (200) GET page 116\n",
      "2024-10-19 11:12:32 [INFO] (200) GET page 117\n",
      "2024-10-19 11:12:43 [INFO] (200) GET page 118\n",
      "2024-10-19 11:12:54 [INFO] (200) GET page 119\n",
      "2024-10-19 11:13:04 [INFO] (200) GET page 120\n",
      "2024-10-19 11:13:15 [INFO] (200) GET page 121\n",
      "2024-10-19 11:13:26 [INFO] (200) GET page 122\n",
      "2024-10-19 11:13:37 [INFO] (200) GET page 123\n",
      "2024-10-19 11:13:46 [INFO] (200) GET page 124\n",
      "2024-10-19 11:13:58 [INFO] (200) GET page 125\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Mengatur logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Header untuk user agent agar mirip dengan browser\n",
    "user_agent = (\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) '\n",
    "    'AppleWebKit/537.36 (KHTML, seperti Gecko) '\n",
    "    'Chrome/106.0.0.0 Safari/537.36'\n",
    ")\n",
    "\n",
    "default_headers = {'User-Agent': user_agent}\n",
    "\n",
    "def get_max_page() -> int:\n",
    "    \"\"\" Mendapatkan jumlah maksimum halaman \"\"\"\n",
    "    session = requests.Session()\n",
    "    response = session.get('https://pergikuliner.com/restaurants?utf8=%E2%9C%93&search_place=&default_search=Bandung&search_name_cuisine=kafe&commit=')\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = page.find('h2', {'id': 'top-total-search-view'})\n",
    "    text = text.find('strong').text.split('dari')\n",
    "    page_num = [int(t.strip()) for t in text]\n",
    "    return int(page_num[1] / page_num[0])\n",
    "\n",
    "def scrape_page(response):\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    cards = page.find_all('div', class_='restaurant-result-wrapper')\n",
    "    data = []\n",
    "    for item in cards:\n",
    "        # judul dan tautan\n",
    "        title = item.find('h3').text.strip()\n",
    "        link = 'https://pergikuliner.com' + item.find('a')['href']\n",
    "        # jenis makanan dan lokasi\n",
    "        description = item.find('div', class_='item-group').find('div').text.strip()\n",
    "        description = description.split('|')\n",
    "        if len(description) > 1:\n",
    "            location = description[0].strip()\n",
    "            cuisine = [i.strip() for i in description[1].split(',')]\n",
    "        else:\n",
    "            location = description\n",
    "            cuisine = None\n",
    "        # peringkat\n",
    "        full_rate = item.find('div', class_='item-rating-result').find('small').text.strip()\n",
    "        rate = item.find('div', class_='item-rating-result').text.replace(full_rate, '').strip()\n",
    "        full_rate = full_rate.replace('/', '')\n",
    "        # lokasi dan harga\n",
    "        for p in item.find_all('p', class_='clearfix'):\n",
    "            if 'icon-map' in p.find('i')['class']:\n",
    "                place = p.find_all('span', class_='truncate')\n",
    "                address = place[0].text.strip()\n",
    "                street = place[1].text.strip()\n",
    "            elif 'icon-price' in p.find('i')['class']:\n",
    "                price_text = p.find('span').text.strip()\n",
    "                if re.findall(r'-', price_text):\n",
    "                    price_from = price_text.split('-')[0].strip()\n",
    "                    price_till = price_text.split('-')[1].replace('/orang', '').strip()\n",
    "                elif re.findall(r'Di atas', price_text):\n",
    "                    price_from = price_text.replace('Di atas', '').replace('/orang', '').strip()\n",
    "                    price_till = None\n",
    "                elif re.findall(r'Di bawah', price_text):\n",
    "                    price_from = 'Rp. 0'\n",
    "                    price_till = price_text.replace('Di bawah', '').replace('/orang', '').strip()\n",
    "                else:\n",
    "                    logging.info(f\"Kondisi lain pada harga\")\n",
    "                    price_from = price_text\n",
    "                    price_till = price_text\n",
    "            else:\n",
    "                logging.info(f\"Bagian lain di lokasi dan harga\")\n",
    "\n",
    "        item_data = {\n",
    "            'title': title,\n",
    "            'rate': rate,\n",
    "            'cuisine': cuisine,\n",
    "            'location': location,\n",
    "            'address': address,\n",
    "            'street': street,\n",
    "            'price_from': price_from,\n",
    "            'price_till': price_till,\n",
    "            'url': link,\n",
    "        }\n",
    "        data.append(item_data)\n",
    "    return data\n",
    "\n",
    "def scrape_restaurant_details(link):\n",
    "    \"\"\" Mengambil detail tambahan dari halaman restoran \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update(default_headers)\n",
    "    try:\n",
    "        response = session.get(link)\n",
    "        if response.status_code == 200:\n",
    "            page = BeautifulSoup(response.text, 'html.parser')\n",
    "            additional_info = {}\n",
    "            \n",
    "            # Mengambil deskripsi restoran\n",
    "            description = page.find('div', class_='restaurant-description')\n",
    "            additional_info['description'] = description.text.strip() jika description else None\n",
    "            \n",
    "            # Mengambil jam buka\n",
    "            opening_hours = page.find('time', itemprop='openingHours')\n",
    "            additional_info['opening_hours'] = opening_hours.text.strip() jika opening_hours else None\n",
    "            \n",
    "            # Memeriksa ketersediaan Wi-Fi\n",
    "            wifi_checkbox = page.find('input', {'type': 'checkbox', 'name': 'wifi'})\n",
    "            additional_info['wifi_available'] = wifi_checkbox.get('checked') is not None jika wifi_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan area merokok\n",
    "            smoking_area_checkbox = page.find('input', {'type': 'checkbox', 'name': 'smoking_area'})\n",
    "            additional_info['smoking_area_available'] = smoking_area_checkbox.get('checked') is not None jika smoking_area_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan tempat duduk luar ruangan\n",
    "            outdoor_seat_checkbox = page.find('input', {'type': 'checkbox', 'name': 'outdoor_seat'})\n",
    "            additional_info['outdoor_seat_available'] = outdoor_seat_checkbox.get('checked') is not None jika outdoor_seat_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan layanan full-time\n",
    "            full_time_checkbox = page.find('input', {'type': 'checkbox', 'name': 'full_time'})\n",
    "            additional_info['full_time_available'] = full_time_checkbox.get('checked') is not None jika full_time_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan ruang VIP\n",
    "            vip_room_checkbox = page.find('input', {'type': 'checkbox', 'name': 'vip_room'})\n",
    "            additional_info['vip_room_available'] = vip_room_checkbox.get('checked') is not None jika vip_room_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan reservasi\n",
    "            reservation_checkbox = page.find('input', {'type': 'checkbox', 'name': 'reservation'})\n",
    "            additional_info['reservation_available'] = reservation_checkbox.get('checked') is not None jika reservation_checkbox else False\n",
    "            \n",
    "            # Memeriksa ketersediaan area parkir\n",
    "            parking_area_checkbox = page.find('input', {'type': 'checkbox', 'name': 'parking_area'})\n",
    "            additional_info['parking_area_available'] = parking_area_checkbox.get('checked') is not None jika parking_area_checkbox else False\n",
    "            \n",
    "            # Mengambil review body untuk review_11, review_12, dan review_13\n",
    "            for review_id in ['review_11', 'review_12', 'review_13']:\n",
    "                review_body = page.find('div', {'id': review_id, 'itemprop': 'reviewBody'})\n",
    "                additional_info[review_id] = review_body.text.strip() jika review_body else 'None'\n",
    "            \n",
    "            # Pastikan jika review_11 ada, review_12 dan review_13 di-set ke 'None' jika tidak ditemukan\n",
    "            jika additional_info['review_11'] == 'None':\n",
    "                additional_info['review_12'] = 'None'\n",
    "                additional_info['review_13'] = 'None'\n",
    "                \n",
    "            return additional_info\n",
    "        else:\n",
    "            logging.warning(f\"Gagal mengambil detail untuk {link} (Status Code: {response.status_code})\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Kesalahan mengambil detail untuk {link}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def crawl(npage=None):\n",
    "    session = requests.Session()\n",
    "    session.headers.update(default_headers)\n",
    "\n",
    "    jika npage is None:\n",
    "        npage = get_max_page() + 1\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for n in range(1, npage):\n",
    "        params = {'page': n}\n",
    "        try:\n",
    "            response = session.get('https://pergikuliner.com/restaurants?utf8=%E2%9C%93&search_place=&default_search=Bandung&search_name_cuisine=kafe&commit=', params=params)\n",
    "            logging.info(f\"({response.status_code}) GET halaman {n}\")\n",
    "            page_data = scrape_page(response)\n",
    "            for restaurant in page_data:\n",
    "                # Mengambil detail tambahan untuk setiap restoran\n",
    "                additional_data = scrape_restaurant_details(restaurant['url'])\n",
    "                restaurant.update(additional_data)\n",
    "            data += page_data\n",
    "            sleep(1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Kesalahan pada {n}: {e}\")\n",
    "            pass\n",
    "    return data\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = crawl()\n",
    "    save_data(data, \"Bandung.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSON file into DataFrame\n",
    "df = pd.read_json('Bandung.json')\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('Bandung.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
